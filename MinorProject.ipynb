{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MinorProject.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDZyLGVRLHudE5zieMkPQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivang1234gupta/MinorProject/blob/master/MinorProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeY0hvLOOh-Z",
        "colab_type": "text"
      },
      "source": [
        "Taking the positive and negative reviews of 2 politician: Politican X and Politician Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhcDJo7IBh8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f555aac4-0812-415b-884a-bbe2a7475df2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NA3hedYOt7_",
        "colab_type": "text"
      },
      "source": [
        "Working for politician X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPoNFTp2gXaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('drive/My Drive/PoliticalLeaderX.txt', 'r') as f2:\n",
        "    data = f2.read()\n",
        "    #print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ZB4-ybDjjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "826638e9-5349-4593-e707-3607c0896ae6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMWyUPc-Dm1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences=nltk.sent_tokenize(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjTK-45bFqsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  textblob import TextBlob\n",
        "blob = TextBlob(data.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnfrfncKGeCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "header_name = ['labels', 'data']\n",
        "with open('sentiment.csv', 'w') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
        "    writer.writeheader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHEDomPHDpEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in blob.sentences:\n",
        "  #print(sent.sentiment.polarity)\n",
        "  with open('sentiment.csv', 'a') as file:\n",
        "    pol=sent.sentiment.polarity\n",
        "    if(pol<0):\n",
        "      pol=-1\n",
        "    else:\n",
        "      pol=1\n",
        "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
        "    info = {\n",
        "                'labels': pol,\n",
        "                'data': sent\n",
        "            }\n",
        "    writer.writerow(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CcI7cebE1mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkTX5Gbsdm-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "685dafdf-7bef-4fd2-ff5d-48078f46f00d"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVD9H-CYdqVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXLP1n8Gii4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('sentiment.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT3f3ydji49S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57e2381e-246e-40d1-c9ee-8cbc50e423f1"
      },
      "source": [
        "type(df['data'][8])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqaFwEHfDIhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "5de8e9d8-acd9-4e70-dfdd-c19f57d8cb38"
      },
      "source": [
        "df.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of        labels                                               data\n",
              "0           1  india s shock gdp growth rate is a crisis modi...\n",
              "1          -1  most analysts had   belatedly   forecast the b...\n",
              "2           1  it is now clear that if the government does no...\n",
              "3           1  the economy is on a cusp from where it can swi...\n",
              "4           1  nirmala sitharaman is on test.if we get past t...\n",
              "...       ...                                                ...\n",
              "14356      -1      because the advertising market is broken too.\n",
              "14357       1  if you think we deserve your support  do join ...\n",
              "14358       1  your support will define our journalism  and t...\n",
              "14359      -1      it will take just a few seconds of your time.\n",
              "14360       1            support our journalismshow full article\n",
              "\n",
              "[14361 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMEFURO6gT3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "allowed_word_types = [\"J\"]\n",
        "all_words=[]\n",
        "for p in  df['data']:\n",
        "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
        "    \n",
        "    tokenized = word_tokenize(cleaned)\n",
        "    \n",
        "    stopped = [w for w in tokenized if not w in stopwords.words('english')]\n",
        "    \n",
        "    pos = nltk.pos_tag(stopped)\n",
        "    \n",
        "    for w in pos:\n",
        "        if w[1][0] in allowed_word_types:\n",
        "            all_words.append(w[0].lower())\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwGizdOgt2wE",
        "colab_type": "text"
      },
      "source": [
        "Bag of words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUjFyWNLkDo1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a frequency distribution of each adjectives.\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "# listing the 1000 most frequent words\n",
        "word_features = list(all_words.keys())[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBEskg9ujo7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_features(document):\n",
        "    words = word_tokenize(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vpXVwk2y7Cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17d086f6-31df-4fd7-8080-8a3582b7b443"
      },
      "source": [
        "import random\n",
        "dataset=[]\n",
        "i=0\n",
        "for p in df['data']:\n",
        "  feature=find_features(p)\n",
        "  dataset.append([feature,df['labels'][i]])\n",
        "  i+=1\n",
        "random.shuffle(dataset)\n",
        "\n",
        "print(len(dataset))# dataset is a bag of model of vectors\n",
        "\n",
        "training_set = dataset[:10000]\n",
        "testing_set = dataset[10000:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r51Tz976jgw",
        "colab_type": "text"
      },
      "source": [
        "Machine Learning Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4OOb3HMLE58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0213bf2-9b60-4b1a-d821-ec4554721681"
      },
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier accuracy percent: 86.44806237101582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q3W6qfMQHDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d06190b6-48a8-4e35-d074-7c66cdb8f69c"
      },
      "source": [
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# training various models by passing in the sklearn models into the SklearnClassifier from NLTK \n",
        "\n",
        "MNB_clf = SklearnClassifier(MultinomialNB())\n",
        "MNB_clf.train(training_set)\n",
        "\n",
        "BNB_clf = SklearnClassifier(BernoulliNB())\n",
        "BNB_clf.train(training_set)\n",
        "\n",
        "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
        "LogReg_clf.train(training_set)\n",
        "\n",
        "SVC_clf = SklearnClassifier(SVC())\n",
        "SVC_clf.train(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v1QviP1QHl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "bf914a2c-eefa-4bc5-adbe-2a068ed7ebf5"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "ground_truth = [r[1] for r in testing_set]\n",
        "predictions = []\n",
        "f1_scores = {}\n",
        "Li={0:MNB_clf,1:BNB_clf,2:LogReg_clf,3:SVC_clf}\n",
        "print(Li[0])\n",
        "for x in Li:\n",
        "  predictions=[]\n",
        "  for r in testing_set:\n",
        "    predictions.append(Li[x].classify(r[0]))\n",
        "  if(x==0):\n",
        "    str='Multinomial Naive Baeyes'\n",
        "  elif(x==1):\n",
        "    str='Bernoulli Naive Baeyes'\n",
        "  elif(x==2):\n",
        "    str='Logistic Regression'\n",
        "  else:\n",
        "    str='Support Vector'\n",
        "  print(str,'classifier accuracy :',accuracy_score(ground_truth,predictions))\n",
        "  print(str,'f1 score :',f1_score(ground_truth,predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>\n",
            "Multinomial Naive Baeyes classifier accuracy : 0.8651685393258427\n",
            "Multinomial Naive Baeyes f1 score : 0.9203899268887085\n",
            "Bernoulli Naive Baeyes classifier accuracy : 0.8601238248108232\n",
            "Bernoulli Naive Baeyes f1 score : 0.9170068027210885\n",
            "Logistic Regression classifier accuracy : 0.8791561568447603\n",
            "Logistic Regression f1 score : 0.929479459387127\n",
            "Support Vector classifier accuracy : 0.8793854620499886\n",
            "Support Vector f1 score : 0.9306800210859252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-0tV-hcFpHj",
        "colab_type": "text"
      },
      "source": [
        "So we clearly see that Logistic regression classifier and Support vector perfectly classifies our dataset\n",
        "But since SVM has more f1-score so we will make predictions based on SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dPyRV2s7UwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions=[]\n",
        "for r in testing_set:\n",
        "  predictions.append(SVC_clf.classify(r[0]))\n",
        "print(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUC3fTEAYfKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive=0\n",
        "negative=0\n",
        "for i in range(0,len(predictions)):\n",
        "  if predictions[i]==1:\n",
        "    positive=positive+1\n",
        "  else:\n",
        "    negative=negative+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJpUyGp9aMiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bb5aa2b-74cd-48bd-ce48-a00097e5ebd0"
      },
      "source": [
        "print(positive,negative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4030 331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T62gdX3ajLy",
        "colab_type": "text"
      },
      "source": [
        "Sentimental Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AERo3yKAaQAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_percent=positive/(positive+negative)\n",
        "negative_percent=negative/(positive+negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfr7yL_9axH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3eb5097-46de-4adf-fc37-79a1206869de"
      },
      "source": [
        "print(positive_percent*100,negative_percent*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92.40999770694795 7.590002293052052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnqttI86bESR",
        "colab_type": "text"
      },
      "source": [
        "So nearly 92% people are in favour of politician X and around 8% people do not favour him"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urTuFq9OBbeA",
        "colab_type": "text"
      },
      "source": [
        "Taking new Data for another politician (politician Y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQhqemxga7cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=''\n",
        "with open('drive/My Drive/PoliticalLeaderY.txt', 'r') as f2:\n",
        "    data = f2.read()\n",
        "    #print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMaTwMO0qwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  textblob import TextBlob\n",
        "blob = TextBlob(data.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2R6RgpZ3-mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "header_name = ['labels', 'data']\n",
        "with open('sentiment1.csv', 'w') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
        "    writer.writeheader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeauwrd64Jk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in blob.sentences:\n",
        "  #print(sent.sentiment.polarity)\n",
        "  with open('sentiment1.csv', 'a') as file:\n",
        "    pol=sent.sentiment.polarity\n",
        "    if(pol<0):\n",
        "      pol=-1\n",
        "    else:\n",
        "      pol=1\n",
        "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
        "    info = {\n",
        "                'labels': pol,\n",
        "                'data': sent\n",
        "            }\n",
        "    writer.writerow(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwsFN_5z4OaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=[]\n",
        "import pandas as pd\n",
        "df=pd.read_csv('sentiment1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8bfcTe35etj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "27fc2b7a-cdb2-48e1-97fd-2519b4b89992"
      },
      "source": [
        "df.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of        labels                                               data\n",
              "0           1         no cure yet for covid  or congress crisis.\n",
              "1           1  rahul gandhi continuously fails to win india\\n...\n",
              "2           1  just as there is no single medicine to cure co...\n",
              "3           1  in the absence of any vaccine  social distanci...\n",
              "4           1  similarly  the need for a strong central leade...\n",
              "...       ...                                                ...\n",
              "19215      -1      because the advertising market is broken too.\n",
              "19216       1  if you think we deserve your support  do join ...\n",
              "19217       1  your support will define our journalism  and t...\n",
              "19218      -1      it will take just a few seconds of your time.\n",
              "19219       1            support our journalismshow full article\n",
              "\n",
              "[19220 rows x 2 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkj8c0b-5nvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "allowed_word_types = [\"J\"]\n",
        "all_words=[]\n",
        "for p in  df['data']:\n",
        "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
        "    \n",
        "    tokenized = word_tokenize(cleaned)\n",
        "    \n",
        "    stopped = [w for w in tokenized if not w in stopwords.words('english')]\n",
        "    \n",
        "    pos = nltk.pos_tag(stopped)\n",
        "    \n",
        "    for w in pos:\n",
        "        if w[1][0] in allowed_word_types:\n",
        "            all_words.append(w[0].lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLrISylBky8",
        "colab_type": "text"
      },
      "source": [
        "Making Bag of words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Aw2zZ15u9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a frequency distribution of each adjectives.\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "# listing the 1000 most frequent words\n",
        "word_features = list(all_words.keys())[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya53Apmr54cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_features(document):\n",
        "    words = word_tokenize(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkFHW-5z58gx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c18022d-0317-407c-dccd-8ee1c821ee64"
      },
      "source": [
        "import random\n",
        "dataset=[]\n",
        "i=0\n",
        "for p in df['data']:\n",
        "  feature=find_features(p)\n",
        "  dataset.append([feature,df['labels'][i]])\n",
        "  i+=1\n",
        "random.shuffle(dataset)\n",
        "\n",
        "print(len(dataset))# dataset is a bag of model of vectors\n",
        "\n",
        "# training_set = dataset[:15000]\n",
        "# testing_set = dataset[15000:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZxs_e7DBsrX",
        "colab_type": "text"
      },
      "source": [
        "Applying Machine Learning Algorithm as trained earlier that is SVM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1opbRFq6B3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions=[]\n",
        "for r in dataset:\n",
        "  predictions.append(SVC_clf.classify(r[0]))\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghezcN_7FeSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ground_truth=[]\n",
        "for r in dataset:\n",
        "  ground_truth.append(r[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKlHwFDJEN7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cb3dc85c-42eb-4f29-8e09-c31ddbcaafaf"
      },
      "source": [
        "print(str,'classifier accuracy :',accuracy_score(ground_truth,predictions))\n",
        "print(str,'f1 score :',f1_score(ground_truth,predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Support Vector classifier accuracy : 0.8559833506763788\n",
            "Support Vector f1 score : 0.917653358719581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUxS_PnuCj4D",
        "colab_type": "text"
      },
      "source": [
        "Sentimental Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esr7XVC86UlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive=0\n",
        "negative=0\n",
        "for i in range(0,len(predictions)):\n",
        "  if predictions[i]==1:\n",
        "    positive=positive+1\n",
        "  else:\n",
        "    negative=negative+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p985xF4Q6dN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_percent=positive/(positive+negative)\n",
        "negative_percent=negative/(positive+negative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCZDlRGD6hg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "829fcbb8-e3d6-4818-ad85-f6a9f43a0a48"
      },
      "source": [
        "print(positive_percent*100,negative_percent*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94.15712799167534 5.842872008324662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNUOX0c7N1Lr",
        "colab_type": "text"
      },
      "source": [
        "So this politician (politician Y) has also 94% positive reviews and 6 % negative reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdeQdvN86mrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}